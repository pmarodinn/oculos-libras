# Redes Neurais Convolucionais - LIBRAS ðŸ– âœ‹ âœŠ â˜

## **Projeto de Reconhecimento de Gestos em LIBRAS com Ã“culos Automatizado**
Este projeto faz parte de um sistema maior que envolve a construÃ§Ã£o de um Ã³culos automatizado equipado com cÃ¢mera e saÃ­da de som. O objetivo Ã© traduzir gestos da LÃ­ngua Brasileira de Sinais (LIBRAS) para Ã¡udio de forma offline e o mais prÃ³ximo possÃ­vel do tempo real.

---

## **Objetivo**
O projeto implementa uma arquitetura CNN para reconhecer gestos das letras do alfabeto em LIBRAS. A arquitetura segue o padrÃ£o:

```
INPUT => CONV => POOL => CONV => POOL => CONV => POOL => FC => FC => OUTPUT
```

AlÃ©m disso, o sistema gera saÃ­da de Ã¡udio fonÃ©tico para a letra reconhecida, permitindo feedback imediato ao usuÃ¡rio.

---

## **Requisitos**

### **DependÃªncias de Software**
- Python 3.9+
- Conda ou Miniconda (para gerenciamento de ambiente virtual)
- Bibliotecas Python:
  - TensorFlow 2.x
  - OpenCV
  - NumPy
  - pyttsx3 (para texto para fala)
  - Outras dependÃªncias listadas no arquivo `environment.yml`.

### **DependÃªncias de Sistema**
- **Linux/Mac/Windows**: Certifique-se de que os seguintes pacotes estÃ£o instalados no seu sistema:
  - `espeak` (para reproduÃ§Ã£o de Ã¡udio):
    ```bash
    sudo apt-get update
    sudo apt-get install espeak
    ```
  - Webcam funcional conectada ao sistema ou cÃ¢mera integrada ao Ã³culos.

---

## **InstalaÃ§Ã£o**

### **1. Clone o RepositÃ³rio**
Clone o repositÃ³rio para o seu ambiente local:
```bash
git clone https://github.com/seu_usuario/nome_do_repositorio.git
cd nome_do_repositorio
```

### **2. Crie o Ambiente Virtual**
Use o arquivo `environment.yml` para criar o ambiente virtual com todas as dependÃªncias necessÃ¡rias:
```bash
conda env create -f environment.yml
```

### **3. Ative o Ambiente**
Ative o ambiente virtual criado:
```bash
conda activate cnn_libras
```

### **4. Instale DependÃªncias Adicionais**
Se necessÃ¡rio, instale pacotes adicionais fora do `environment.yml` (por exemplo, `espeak` no Linux):
```bash
sudo apt-get install espeak
```

---

## **ExecuÃ§Ã£o**

### **1. Treinamento do Modelo**
Para treinar o modelo usando o dataset fornecido:
```bash
python main/train.py
```
Certifique-se de que o dataset esteja organizado corretamente nas pastas `dataset/training` e `dataset/test`.

### **2. ExecuÃ§Ã£o em Tempo Real**
Para testar o modelo em tempo real com sua webcam ou cÃ¢mera integrada ao Ã³culos:
```bash
python main/app_64x64x3.py
```
O sistema exibirÃ¡ a imagem capturada pela cÃ¢mera.
Quando uma letra for reconhecida, o som fonÃ©tico serÃ¡ emitido automaticamente.

### **3. Teste com Imagens EstÃ¡ticas**
VocÃª tambÃ©m pode testar o modelo com imagens estÃ¡ticas:
```bash
python main/app_imgpath.py /caminho/para/imagem.png
```

---

## **Estrutura do Projeto**

```
â”œâ”€â”€ dataset/               # Dataset e scripts para processamento de imagens
â”‚   â”œâ”€â”€ pre-processed/     # Imagens prÃ©-processadas (treinamento e teste)
â”‚   â”œâ”€â”€ training/          # Dataset de treinamento
â”‚   â”œâ”€â”€ test/              # Dataset de teste
â”‚   â”œâ”€â”€ resize_img.py      # Script para redimensionar imagens
â”‚   â””â”€â”€ capture.py         # Script para capturar imagens da cÃ¢mera
â”œâ”€â”€ demo/                  # DemonstraÃ§Ã£o do projeto (GIFs, vÃ­deos, etc.)
â”œâ”€â”€ logs/                  # Logs de execuÃ§Ã£o (histÃ³rico de treinamento, etc.)
â”œâ”€â”€ main/                  # CÃ³digo principal do projeto
â”‚   â”œâ”€â”€ cnn/               # ImplementaÃ§Ã£o da CNN
â”‚   â”‚   â””â”€â”€ __init__.py    # DefiniÃ§Ã£o da arquitetura da CNN
â”‚   â”œâ”€â”€ train.py           # Script para treinar o modelo
â”‚   â”œâ”€â”€ app_64x64x3.py     # Script para reconhecimento em tempo real
â”‚   â””â”€â”€ app_imgpath.py     # Script para reconhecimento com imagens estÃ¡ticas
â”œâ”€â”€ models/                # Modelos treinados e grÃ¡ficos
â”‚   â”œâ”€â”€ graphics/          # GrÃ¡ficos gerados durante o treinamento
â”‚   â”œâ”€â”€ image/             # RepresentaÃ§Ãµes visuais da arquitetura da CNN
â”‚   â””â”€â”€ model_epoch_61.h5  # Exemplo de modelo treinado
â”œâ”€â”€ temp/                  # Arquivos temporÃ¡rios (imagens capturadas pela cÃ¢mera)
â”œâ”€â”€ environment.yml        # Arquivo para criar o ambiente Conda
â””â”€â”€ README.md              # Este arquivo
```

---

## **ReferÃªncias**

### **DocumentaÃ§Ã£o e Tutoriais**
- **Arquitetura CNN**: [CS231n - Convolutional Neural Networks](http://cs231n.stanford.edu/)
- **Keras**: [DocumentaÃ§Ã£o Oficial do Keras](https://keras.io/)
- **TensorFlow**: [DocumentaÃ§Ã£o Oficial do TensorFlow](https://www.tensorflow.org/)

### **Ferramentas Usadas**
- **OpenCV**: [DocumentaÃ§Ã£o Oficial do OpenCV](https://opencv.org/)
- **pyttsx3**: [Texto para Fala em Python](https://pyttsx3.readthedocs.io/en/latest/)
- **Conda**: [Gerenciamento de Ambientes com Conda](https://docs.conda.io/en/latest/)

---


